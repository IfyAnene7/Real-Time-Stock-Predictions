{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis of the US Stock Financial indicators data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names of contributors: Anene Ifeanyi, Chizitere Igwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date: 2020-12-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "1. [Summary](#Summary)\n",
    "2. [Method](#Method)\n",
    "3. [Data](#Data)\n",
    "4. [Partition Data into train and test splits](#PartitionDataintotrainandtestsplits)\n",
    "5. [Exploratory Data Visualisations](#ExploratoryDataVisualisations)\n",
    "6. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary <a name=\"Summary\"></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method <a name=\"Method\"></a>\n",
    "\n",
    "Given the financial stock indicators, should a hypothetical investor buy the stock or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data <a name=\"Data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required exploratory data analysis packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# train test split and cross validation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, RandomizedSearchCV\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "\n",
    "# classifiers / models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, RidgeCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Others\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.metrics import f1_score, mean_squared_error, make_scorer, recall_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this exploratory data analysis, we will only be working with the 2014 dataset. The techniques applied here can be applied to the other datasets.\n",
    "\n",
    "df_2014 = pd.read_csv('../data/raw/2014_Financial_Data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = df_2014.rename(columns={'Unnamed: 0': 'Ticker'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Results <a name=\"PartitionDataintotrainandtestsplits\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_2014, train_size = 0.75, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y train\n",
    "\n",
    "X_train, y_train = (train_df.drop(columns = [\"Class\"]), train_df[\"Class\"])\n",
    "\n",
    "X_test, y_test = (test_df.drop(columns = [\"Class\"]), test_df[\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and transformations\n",
    "\n",
    "drop_features = [\"Ticker\"]\n",
    "\n",
    "categorical_features = [\"Sector\"]\n",
    "\n",
    "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "#len(numerical_features) == 222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Ticker` column was dropped because it does not seem to add any significant contribution to prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(StandardScaler(), SimpleImputer())\n",
    "\n",
    "categroical_transformer = make_pipeline(SimpleImputer(strategy = \"constant\", fill_value = \"missing\"),\n",
    "                                       OneHotEncoder(handle_unknown = \"ignore\"))\n",
    "\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numerical_features),\n",
    "    (categorical_tranformer, categorical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function \n",
    "\n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the mean and standard deviation of cross validation scores\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "    \n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    \n",
    "    out_col = []\n",
    "    \n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "    \n",
    "    return pd.Series(data = out_col, index = mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "scoring_metric = [\"accuracy\", \"recall\", \"precision\", \"f1\"]\n",
    "\n",
    "dummy_model = make_pipeline(preprocessor, DummyClassifier(strategy = \"stratified\"));\n",
    "\n",
    "results[\"dummy\"] = mean_std_cross_val_scores(dummy_model, X_train, y_train, return_train_score = True, scoring = scoring_metric);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1: Results of the baseline model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1 indicates the results of the baseline model developed. The scores of this model are quite low, however, they can be used as a reference for the models that will be later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "log_reg_model = make_pipeline(preprocessor, LogisticRegression(max_iter = 1000, class_weight = \"balanced\"))\n",
    "\n",
    "results[\"Logistic Regression\"] = mean_std_cross_val_scores(log_reg_model, X_train, y_train, return_train_score = True, \n",
    "                                                           scoring = scoring_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2: Results of baseline model and Logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model shows good promise. The scores are better than the dummy classifier, however, the scores are still pretty low. Optimising the regularisation (`C`) hyperparameter should give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_param_grid = {\"logisticregression__C\": 10.0 ** np.arange(-3, 3, 1)}\n",
    "mult_metric_eval_scorer = {\"accuracy\" : \"accuracy\", \"recall\" : \"recall\", \"precision\" : \"precision\", \"f1\" : \"f1\"}\n",
    "\n",
    "log_reg_random_search = RandomizedSearchCV(log_reg_model, param_distributions = log_reg_param_grid,\n",
    "                                           n_jobs = -1, verbose = 1, scoring = mult_metric_eval_scorer, refit = \"f1\",\n",
    "                                           return_train_score = True);\n",
    "\n",
    "log_reg_random_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_log_reg_model = log_reg_random_search.best_estimator_;\n",
    "\n",
    "results[\"Logistic Regression (tuned)\"] = mean_std_cross_val_scores(best_log_reg_model, X_train, y_train, return_train_score = True,\n",
    "                                                      scoring = scoring_metric);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 3: Results of Baseline, Logistic regression, and regularisation optimised logistic regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimising the regularisation hyperparameter, much better scores for each evaluation metric is obtained. I still think this can be improved with automatic feature selection. However, I intend to explore other classifiers before attempting feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out ridgeclassifier and ensembles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"RBF SVM\": SVC(),\n",
    "    \"random forest\": RandomForestClassifier(class_weight=\"balanced\", random_state=2),\n",
    "    \"xgboost\": XGBClassifier(scale_pos_weight=ratio, random_state=2),\n",
    "    \"lgbm\": LGBMClassifier(scale_pos_weight=ratio, random_state=2),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = make_pipeline(preprocessor, model)\n",
    "    results[name] = mean_std_cross_val_scores(\n",
    "        pipe, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    "    )\n",
    "\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a name=\"References\"></a>\n",
    "\n",
    "de Jonge, E., 2020. CRAN - Package Docopt. [online] Cran.r-project.org. Available at: https://cran.r-project.org/web/packages/docopt/index.html [Accessed 29 November 2020].\n",
    "\n",
    "Oliphant, T.E., 2006. A guide to NumPy, Trelgol Publishing USA.\n",
    "\n",
    "McKinney, W. & others, 2010. Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference. pp. 51–56.\n",
    "\n",
    "Waskom, M. et al., 2017. mwaskom/seaborn: v0.8.1 (September 2017), Zenodo. Available at: https://doi.org/10.5281/zenodo.883859.\n",
    "\n",
    "Van Rossum, G. & Drake, F.L., 2009. Python 3 Reference Manual, Scotts Valley, CA: CreateSpace.\n",
    "\n",
    "Hunter, J.D., 2007. Matplotlib: A 2D graphics environment. Computing in science & engineering, 9(3), pp.90–95.\n",
    "\n",
    "Pedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825–2830.\n",
    "\n",
    "P'erez, Fernando & Granger, B.E., 2007. IPython: a system for interactive scientific computing. Computing in Science & Engineering, 9(3).\n",
    "\n",
    "Kluyver, T. et al., 2016. Jupyter Notebooks – a publishing format for reproducible computational workflows. In F. Loizides & B. Schmidt, eds. Positioning and Power in Academic Publishing: Players, Agents and Agendas. pp. 87–90.\n",
    "\n",
    "Anon, 2020. Anaconda Software Distribution, Anaconda Inc. Available at: https://docs.anaconda.com/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
